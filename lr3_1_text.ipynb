{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "4K0ZgdqlVyRY"
   },
   "source": [
    "# Text Classification\n",
    "\n",
    "In this problem, you will be again analyzing the Twitter data we extracted from 2016 using [this](https://dev.twitter.com/overview/api) api. This time, we extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`.\n",
    "\n",
    "For every tweet, we collected two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "We divided the tweets into two parts - the train and test sets.  The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`.\n",
    "\n",
    "The overarching goal of the problem is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n",
    "- **R**: `realDonaldTrump, mike_pence, GOP`\n",
    "- **D**: `HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "We can treat this as a binary classification problem. We'll follow this common structure to tackling this problem:\n",
    "\n",
    "1. **preprocessing**: clean up the raw tweet text using regular expressions, and produce class labels\n",
    "2. **features**: construct bag-of-words feature vectors\n",
    "3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utCwhv4TVyRa"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.svm\n",
    "import sklearn.metrics\n",
    "import gzip\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "uW4q7e55VyRb"
   },
   "source": [
    "## A. Text Processing\n",
    "\n",
    "### Q1 Preprocessing\n",
    "Your first task is to fill in the following function which processes and tokenizes raw text. You will need to preprocess the tokens by applying the operators _in the following order_.\n",
    "\n",
    "1. Convert the text to lower case.\n",
    "2. Remove any URLs, which in this case will all be of the form `http://t.co/<alphanumeric characters>`.\n",
    "3. Remove all trailing `'s` characters, followed by other apostrophes:\n",
    "   - remove trailing `'s`: `Children's` becomes `children`\n",
    "   - omit other apostrophes: `don't` becomes `dont`\n",
    "4. Remove all non-alphanumeric (i.e., A-Z, a-z, 0-9) characters (replacing them with a single space)\n",
    "5. Split the remaining text by whitespace into an array of individual words\n",
    "6. Discard empty strings (i.e., if the string after processing above is equal to \"\"), return an empty array `[]` rather than `['']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc13YS0CVyRc"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    \n",
    "    args:\n",
    "        text: str -- raw text\n",
    "\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http://t\\.co/\\w+\", \"\", text)\n",
    "    text = re.sub(r\"'s\\b\", \"\", text)\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDhP3WOEVyRd"
   },
   "source": [
    "### Q2 Loading Data\n",
    "\n",
    "Using this preprocess function, load the data from the relevant csv files and return a list of the parsed tweets, plus a flag indicating whether or not the tweet is from a republican (i.e., one of the three usernames mentioned above); for the test data, where no screen name is given, provide `None` as the flag).  Note that this function should take less than a second if you've implemented the above preprocessing function efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNSqIqo3VyRd"
   },
   "outputs": [],
   "source": [
    "# @mugrade.local_tests\n",
    "def read_data():\n",
    "    \"\"\"Reads the dataset from the tweets_train.csv.gz and tweets_test.csv.gz files\n",
    "    \n",
    "    return : Tuple (data_train, data_test)\n",
    "        data_train : List[Tuple[is_republican, tokenized_tweet]]\n",
    "            is_republican : bool -- True if tweet is from a republican\n",
    "            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n",
    "        data_test : List[Tuple[None, tokenized_tweet]\n",
    "            None: the Python constant \"None\"\n",
    "            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n",
    "    \"\"\"\n",
    "    republican_accounts = {\"realDonaldTrump\", \"mike_pence\", \"GOP\"}\n",
    "\n",
    "    train_df = pd.read_csv(\"tweets_train.csv\")\n",
    "    data_train = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        screen_name = row[\"screen_name\"]\n",
    "        is_republican = screen_name in republican_accounts\n",
    "        tokenized_tweet = preprocess(row[\"text\"])\n",
    "        data_train.append((is_republican, tokenized_tweet))\n",
    "\n",
    "    test_df = pd.read_csv(\"tweets_test.csv\")\n",
    "    data_test = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        tokenized_tweet = preprocess(row[\"text\"])\n",
    "        data_test.append((None, tokenized_tweet))\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIsFz7WVVyRe"
   },
   "source": [
    "## B. Feature Construction\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n",
    "\n",
    "\n",
    "### Q3 Word distributions\n",
    "The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic: We calculate a frequency distribution of words in the corpus and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text. Words with extremely low frequency tend to be typos.\n",
    "\n",
    "We will now implement a function that counts the number of times that each token is used in the training corpus. You should return a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object with the number of times that each word appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY2_r_h4VyRe"
   },
   "outputs": [],
   "source": [
    "def get_distribution(data):\n",
    "    \"\"\" Calculates the word count distribution.\n",
    "\n",
    "    args: \n",
    "        data -- the training or testing data\n",
    "\n",
    "    return : collections.Counter -- the distribution of word counts\n",
    "    \"\"\"\n",
    "    word_counts = collections.Counter()\n",
    "    for _, tokenized_tweet in data:\n",
    "        word_counts.update(tokenized_tweet)\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW9bsvDZVyRf"
   },
   "source": [
    "We can use this function, once implemented properly, to get a sense of the distribution of words in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS253KLdVyRg",
    "outputId": "8a764230-513a-44ca-d029-03856c5a8479"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train, data_test = read_data()\n",
    "dist_train = get_distribution(data_train)\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.hist(dist_train.values(), bins=np.logspace(0,4,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HFktDpgVyRg"
   },
   "source": [
    "Notice that the plot looks roughly linear on this log-log plot (for those we are curious, this is a phenomenon known as [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)).  For us, however, it largely means that there are many words that appear many times (i.e., common words like \"the\"), which thus won't be very predictive for our task, because they are unlike to differentiate Republican vs. Democratic tweets.  There are also words that appear very infrequently, which also means that they aren't going to be very predictive, but for a different reason: these words likely won't occur very often in the test set, and thus will largely just cause the classifier to overfit to the training set.  However, instead of removing these words manually, in the next question, we will use the TFIDF weighting and vectorizer to both remove overly-common words and exclude too-infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfakhNfWVyRh"
   },
   "source": [
    "### Q4 Vectorizing\n",
    "\n",
    "Now we have each tweet as a list of words, excluding words with high and low frequencies. We want to convert these into a sparse feature matrix, where each row corresponds to a tweet and each column to a possible word. We can use `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to do this quite easily.\n",
    "\n",
    "Instructions:\n",
    " - By default, the `TfidfVectorizer` does its own tokenization, but we've already done it above, so you need to pass `preprocessor = lambda x : x, tokenization = lambda x : x, token_pattern=None` as arguments to the class constructor.\n",
    " - The vectorizer can filter words that are too uncommon or too common: to do this, set the `min_df=5` argument (words must be contained in more than 5 tweets), and `max_df=0.4` argument (filter out words contained in more than 40% of tweets)\n",
    " - You should use only the training data to `fit` or `fit_transform` the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uf2vVuFVyRh"
   },
   "outputs": [],
   "source": [
    "def create_features(train_data, test_data):\n",
    "    \"\"\"creates the feature matrices and label vector for the training and test sets.\n",
    "\n",
    "    args:\n",
    "        train_data, test_data : output of read_data() function\n",
    "\n",
    "    returns: Tuple[train_features, train_labels, test_features]\n",
    "        train_features : scipy.sparse.csr.csr_matrix -- TFIDF feature matrix for the training set\n",
    "        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat \n",
    "        test_features : scipy.sparse.csr.csr_matrix -- TFIDF feature matrix for the test set\n",
    "    \"\"\"\n",
    "    train_texts = [\" \".join(tokens) for _, tokens in train_data]\n",
    "    train_labels = np.array([1 if is_republican else 0 for is_republican, _ in train_data])\n",
    "\n",
    "    test_texts = [\" \".join(tokens) for _, tokens in test_data]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        preprocessor=lambda x: x,\n",
    "        tokenizer=lambda x: x,\n",
    "        token_pattern=None,\n",
    "        min_df=5,\n",
    "        max_df=0.4\n",
    "    )\n",
    "    train_features = vectorizer.fit_transform(train_texts)\n",
    "    test_features = vectorizer.transform(test_texts)\n",
    "\n",
    "    return train_features, train_labels, test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOLRyBecVyRi"
   },
   "source": [
    "Observe that the created matrices are very sparse, which is to be expected especially for tweets (given that each tweet can only contain relatively few words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxnSx3JQVyRi"
   },
   "source": [
    "## C. Classification\n",
    "\n",
    "We are now ready to put it all together and train the classification model.\n",
    "\n",
    "You will be using the Support Vector Machine [`sklearn.svm.LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn). This class implements a linear SVM as we described in class, though of course, the details vary a little bit with this particular implementation.\n",
    "\n",
    "### Q5 Training a classifier\n",
    "\n",
    "Let's begin by training a classifier. You should specifically train a `LinearSVC` with a given set of features and labels, plus the regularization parameter specified by `C`.  You can additionally include as arguments to the `LinearSVC` class the `loss = \"hinge\"` argument (so that this is a typical SVM), and the `random_state=0` argument (to avoid any randomness in the training).  **Additionally, you should use the `max_iter=10000` argument to make sure that you run for enough iterations to avoid any failure to converge given the regularization parameters we use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx-EVknwVyRi"
   },
   "outputs": [],
   "source": [
    "def train_classifier(features, labels, C):\n",
    "    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n",
    "\n",
    "    args:\n",
    "        features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        C : float -- C regularization parameters\n",
    "\n",
    "    returns: sklearn.svm.LinearSVC -- classifier trained on data\n",
    "    \"\"\"\n",
    "    classifier = LinearSVC(\n",
    "        C=C,\n",
    "        loss=\"hinge\",\n",
    "        random_state=0,\n",
    "        max_iter=10000\n",
    "    )\n",
    "    classifier.fit(features, labels)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYegTyJuVyRi"
   },
   "source": [
    "### Q6 Cross validation\n",
    "\n",
    "After building the function to train this classifier, let's now use a validation set to pick the optimal value of `C`, out of the choices of `(0.01, 0.1, 1.0, 10.0)`.  The basic approach here will be to split the training set into the first 10000 samples for the training set, and the remainder for the validation set, allowing you to choose the best parameter to use on the training set.  To evaluate the quality of the classifier, you will use the [F1 score](https://en.wikipedia.org/wiki/F-score), a common metric for text classification, which you can compute using the `sklearn.metrics.f1_score` function.\n",
    "\n",
    "Specifically, you should implement the function below, which will compute the training and validation F1 score for different classifiers trained with different values of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxIkajtXVyRj"
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(features, labels, C = (0.01, 0.1, 1.0, 10., 100.), train_length=10000):\n",
    "    \"\"\" Train multiple classifier based on the first train_length features of features/labels,\n",
    "        one for each regularization parameter supplied in C, and return train/validation f1\n",
    "        scores for each of the classifiers\n",
    "    \n",
    "    args:\n",
    "        features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        C : Tuple[float] -- tuple of C regularization parameters\n",
    "        train_length: int -- use _first_ train_length features for training (and the rest of validation)\n",
    "    \n",
    "    return : List[Tuple[float, float]] -- list of F1 scores for training/validation for each C parameter\n",
    "    \"\"\"\n",
    "    train_features = features[:train_length]\n",
    "    train_labels = labels[:train_length]\n",
    "    validation_features = features[train_length:]\n",
    "    validation_labels = labels[train_length:]\n",
    "\n",
    "    results = []\n",
    "    for c in C:\n",
    "        classifier = LinearSVC(C=c, loss=\"hinge\", random_state=0, max_iter=10000)\n",
    "        classifier.fit(train_features, train_labels)\n",
    "        train_predictions = classifier.predict(train_features)\n",
    "        validation_predictions = classifier.predict(validation_features)\n",
    "        train_f1 = f1_score(train_labels, train_predictions)\n",
    "        validation_f1 = f1_score(validation_labels, validation_predictions)\n",
    "        results.append((train_f1, validation_f1))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hARWczSdVyRj"
   },
   "source": [
    "### Q7 Classifying new Tweets\n",
    "\n",
    "Finally, let's put this all together.  Using the _best_ `C` value you found in the previous part (i.e., build the classifiers and test which `C` value out of `(0.01, 0.1, 1.0, 10., 100.)` gives the highest F1 score on the _validation_ set (you can hardcode this value into the function below), train a classifier on the _entire_ training set, and make predictions for the test set.  You won't be able to evaluate how accurate these predictions are, of course, but you can use this classifier to classify tweets as being from Republican or Democratic sources (or perhaps more precisely, from being from one of the three aforementioned Republicans or three Democrats during the 2016 election)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEyjZ1fXVyRj"
   },
   "outputs": [],
   "source": [
    "def predict_test(train_features, train_labels, test_features):\n",
    "    \"\"\"train the classifier on the training set and return predictions on the test set\n",
    "    \n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of training features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of training class labels\n",
    "        test_features: scipy.sparse.csr.csr_matrix -- sparse matrix of test set features\n",
    "\n",
    "    return : numpy.ndarray(bool): array of predictions on the test set\n",
    "    \"\"\"\n",
    "    classifier = LinearSVC(C=1.0, loss=\"hinge\", random_state=0, max_iter=10000)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Predict on the test set\n",
    "    test_predictions = classifier.predict(test_features)\n",
    "\n",
    "    return test_predictions"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
